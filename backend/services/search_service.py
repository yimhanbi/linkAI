# Imports
import json
import re
import os
import asyncio
import time
from typing import List,Dict,Tuple,Optional
from contextlib import asynccontextmanager # ì‹œì‘ê³¼ ì¢…ë£Œ ì‹œì ì— íŠ¹ì • ì‘ì—…ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë„êµ¬

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel # ë°ì´í„° ê²€ì¦ ë° ë°ì´í„° ë³€í™˜ 
from openai import AsyncOpenAI
from qdrant_client import AsyncQdrantClient


#--------------------------------------
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • 
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "patents")
JSON_PATH = os.getenv("JSON_PATH")

# ë””ë²„ê·¸ ì„±ëŠ¥ ë¡œê·¸ on/off (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´)
DEBUG_PERF = os.getenv("DEBUG_PERF", "false").lower() == "true"

def perf_log(msg: str):
    """DEBUG_PERF=trueì¼ ë•Œë§Œ ì¶œë ¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if DEBUG_PERF:
        print(msg)



#--------------------------------------
# ì „ì—­ ë³€ìˆ˜ (ì‹œì‘ì‹œ ì´ˆê¸°í™”)
client_openai: Optional[AsyncOpenAI] = None
client_qdrant : Optional[AsyncQdrantClient] = None

#íƒ€ì… íŒíŠ¸ 
patents: List[Dict] = []
patent_index: Dict[str, Dict] = {}
patent_text_index: Dict[str, str] = {}
patent_flattened: List[Dict] = []




#--------------------------------------
#ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def normalize_application_number(app_no):
    if not app_no:
        return None
    return re.sub(r"[^0-9]", "", app_no)


def find_key_recursive(obj, target_key):
    results = []
    
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k == target_key:
                results.append(v)
            # âœ… í‚¤ ë§¤ì¹­ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ê°’ë„ ì¬ê·€ ê²€ìƒ‰
            results.extend(find_key_recursive(v, target_key))
                
    elif isinstance(obj, list):
        for item in obj:
            results.extend(find_key_recursive(item, target_key))
            
    return results

def extract_applicant_names(patent):
    nums = find_key_recursive(patent,"applicationNumber")
    return nums[0] if nums else None


def build_patent_context_ko(patent:dict) -> str:
    def first(v):
        return v[0] if isinstance (v,list) and v else v
    
    app_no = first(find_key_recursive(patent,"applicationNumber"))
    title = first(find_key_recursive(patent,"inventionTitle"))
    abstract = first(find_key_recursive(patent,"astrtCont"))
    
    #ì²­êµ¬í•­ ì „ì²´
    claims = find_key_recursive(patent,"claim")
    claims_text  ='\n\n'.join(
        [f"ì²­êµ¬í•­ {i+1}\n{c}" for i, c in enumerate(claims)]
    )if claims else None
    
    
    #ë°œëª…ì / ì¶œì›ì¸
    inventors = find_key_recursive(patent, "name")
    inventors_text = ", ".join(dict.fromkeys(inventors)) if inventors else None

    applicants = find_key_recursive(patent, "engName")
    if not applicants:
        applicants = find_key_recursive(patent, "name")
    applicants_text = ", ".join(dict.fromkeys(applicants)) if applicants else None

    sections = []

    if app_no:
        sections.append(f"[ì¶œì›ë²ˆí˜¸]\n{app_no}")

    if title:
        sections.append(f"[ë°œëª…ì˜ ëª…ì¹­]\n{title}")

    if abstract:
        sections.append(f"[ìš”ì•½]\n{abstract}")

    if claims_text:
        sections.append(f"[ì²­êµ¬í•­]\n{claims_text}")

    if inventors_text:
        sections.append(f"[ë°œëª…ì]\n{inventors_text}")

    if applicants_text:
        sections.append(f"[ì¶œì›ì¸]\n{applicants_text}")

    return "\n\n".join(sections)


def extract_application_number(patent):
    """íŠ¹í—ˆ ë°ì´í„°ì—ì„œ ì¶œì›ë²ˆí˜¸(applicationnumber)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    nums =find_key_recursive(patent, "applicationNumber")
    return nums[0] if nums else None


#--------------------------------------
#LLM ê´€ë ¨ í•¨ìˆ˜ë“¤ 

async def extract_weighted_keywords_llm(query: str):
    start = time.time()
    resp = await client_openai.chat.completions.create(
        model="gpt-5",
        messages=[
            {
                "role": "user",
                "content": f"""
ë‹¤ìŒ ë¬¸ì¥ì—ì„œ íŠ¹í—ˆ ê²€ìƒ‰ì— **ì§ì ‘ ì‚¬ìš©ë˜ëŠ” ê²€ìƒ‰ ì¡°ê±´ í‚¤ì›Œë“œë§Œ** ì¶”ì¶œí•˜ì„¸ìš”.

ê·œì¹™:
- íŠ¹í—ˆ DBì—ì„œ ê²€ìƒ‰ í•„ë“œ(ex ì¶œì›ì¸/ë°œëª…ì/ê¸°ìˆ ëª… ë“±)ë¡œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ë§Œ í¬í•¨
- ì§ˆë¬¸ì— ë‚˜ì—´ëœ ëª¨ë“  ì¸ë¬¼ê³¼ ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ë„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ê°ê° ë…ë¦½ì ì¸ í–‰ìœ¼ë¡œ ì¶”ì¶œí•  ê²ƒ.    
- 'ì±…ì„ì—°êµ¬ì' , 'êµìˆ˜', 'ë°•ì‚¬' ë“± ì¸ë¬¼ì„ ìˆ˜ì‹í•˜ëŠ” ì—­í• ì–´ë‚˜ ì§ˆë¬¸ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•œ ë‹¨ì–´(ex ê°œìˆ˜, ì´ë¦„, ë¬´ì—‡, ëª‡ ê°œ ë“±)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ
- ì¶œì›ì¸Â·ë°œëª…ì ì´ë¦„Â·ì¶œì›ë²ˆí˜¸ê°€ ì¡´ì¬í•  ê²½ìš° ìµœìš°ì„ 
- ì§ˆë¬¸ì— ì˜¤íƒ€, ë„ì–´ì“°ê¸° ì˜¤ë¥˜, í•œì˜ë³€í™˜ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ê²ƒì´ ìˆë‹¤ë©´ ì •ì œí•˜ì—¬ ë‹µí•˜ì„¸ìš”.
- ë¬¸ì¥ì— ì‹¤ì œ ë“±ì¥í•œ ë‹¨ì–´ë‚˜ ìˆ«ìë§Œ ì‚¬ìš©
- ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
- ê°€ì¤‘ì¹˜ëŠ” 0~1 (0.1 ë‹¨ìœ„)
- í˜•ì‹: ë‹¨ì–´:ê°€ì¤‘ì¹˜
- ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„
- ì„¤ëª… ì—†ì´ ì¶œë ¥

ë¬¸ì¥:
{query}
"""
            }
        ],
    )
    
    raw = resp.choices[0].message.content.strip()
    
    perf_log("\nğŸ§  [RAW LLM OUTPUT]")
    perf_log(raw)
    
    weighted_keywords = []
    
  
    for line in raw.splitlines():
        line = line.strip()
        if not line or ":" not in line:
            continue
        
       
        k, w = line.split(":", 1)
        
        try:
            weight = float(w.strip())
            weighted_keywords.append((k.strip(), weight))
        except ValueError:
            continue  
    
    perf_log(f"â±ï¸ [LLM í‚¤ì›Œë“œ ì¶”ì¶œ] {time.time() - start:.2f}ì´ˆ")
    return weighted_keywords

#--------------------------------------
#ê²€ìƒ‰ ê´€ë ¨ í•¨ìˆ˜ë“¤

async def get_query_embedding(text: str):
    emb = await client_openai.embeddings.create(
        model="text-embedding-3-large",
        input = text
    )
    return emb.data[0].embedding

async def qdrant_search_app_numbers(query:str,limit: int):
    start = time.time()
    
    emb_start = time.time()
    vector = await get_query_embedding(query)
    perf_log(f"â±ï¸ [ì„ë² ë”© ìƒì„±] {time.time() - emb_start:.2f}ì´ˆ")
    
    search_start = time.time()
    results = await client_qdrant.query_points(
        collection_name=COLLECTION_NAME,
        query=vector,
        limit=limit,
        with_payload=True
    )
    perf_log(f"â±ï¸ [Qdrant ì¿¼ë¦¬] {time.time() - search_start:.2f}ì´ˆ")
    
    apps=[]
    for r in results.points:
        raw = r.payload.get("applicationNumber")
        app_no = normalize_application_number(raw)
        if app_no:
            apps.append(app_no)
    
    perf_log(f"â±ï¸ [Qdrant ì „ì²´] {time.time() - start:.2f}ì´ˆ â†’ {len(apps)}ê°œ")
    return apps


async def simple_match_search_app_numbers(query: str, limit: int):
    """
    âœ” LLMì´ ì¤€ ê°€ì¤‘ì¹˜ë¡œ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •
    âœ” ë¬¸ì„œ ì ìˆ˜ëŠ” ê° í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ë¥¼ ë²¡í„°ë¡œ ë§Œë“¤ì–´ ì‚¬ì „ì‹(lexicographic) ë¹„êµë¡œ ì •ë ¬
    """
    start = time.time()
    perf_log(f"\n{'='*60}")
    perf_log(f"ğŸ” [SIMPLE MATCH SEARCH START]")
    perf_log(f"   Query: '{query}'")
    perf_log(f"   Limit: {limit}")
    perf_log(f"{'='*60}")
    
    weighted_keywords = await extract_weighted_keywords_llm(query)
    perf_log(f"\nğŸ” [LLM WEIGHTED KEYWORDS] â†’ {weighted_keywords}")
    
    if not weighted_keywords:
        print("âŒ No weighted keywords extracted!")
        return []
    
    # âœ… 1. ê°€ì¤‘ì¹˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ì¤‘ìš” í‚¤ì›Œë“œ ìš°ì„ )
    weighted_keywords = sorted(weighted_keywords, key=lambda x: x[1], reverse=True)
    perf_log(f"ğŸ” [SORTED KEYWORDS] â†’ {weighted_keywords}")
    
    perf_log(f"\nğŸ” [DATA CHECK]")
    perf_log(f"   patent_flattened length: {len(patent_flattened)}")
    perf_log(f"   patent_flattened type: {type(patent_flattened)}")
    
    if not patent_flattened:
        print("âŒ ERROR: patent_flattened is empty!")
        return []
    
    # ğŸ” ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸ (ë””ë²„ê·¸ìš© - ì£¼ì„ ì²˜ë¦¬)
    # print(f"\nğŸ“„ [FIRST PATENT SAMPLE]")
    # first_patent = patent_flattened[0]
    # print(f"   app_no: {first_patent['app_no']}")
    # print(f"   text length: {len(first_patent['text'])}")
    # print(f"   text preview (first 300 chars):\n{first_patent['text'][:300]}")
    
    # ğŸ” í‚¤ì›Œë“œê°€ ì²« ë²ˆì§¸ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸ (ë””ë²„ê·¸ìš© - ì£¼ì„ ì²˜ë¦¬)
    # print(f"\nğŸ” [KEYWORD CHECK IN FIRST PATENT]")
    # for keyword, weight in weighted_keywords:
    #     count = first_patent['text'].count(keyword)
    #     print(f"   '{keyword}': {count} occurrences")
    #     if count > 0:
    #         idx = first_patent['text'].find(keyword)
    #         context = first_patent['text'][max(0, idx-50):idx+len(keyword)+50]
    #         print(f"      Context: ...{context}...")
    
    scored = []
    
    # print(f"\nğŸ” [SCANNING ALL PATENTS]")
    # print(f"   Total patents to scan: {len(patent_flattened)}")
    
    matched_patents = 0
    
    for i, p in enumerate(patent_flattened):
        text = p["text"]
        
        # âœ… 2. í‚¤ì›Œë“œë³„ ë“±ì¥ íšŸìˆ˜ ë²¡í„°
        count_vector = tuple(text.count(k) for k, _ in weighted_keywords)
        
        # ğŸ” ì²˜ìŒ 5ê°œ íŠ¹í—ˆëŠ” ìƒì„¸ ë¡œê·¸ (ë””ë²„ê·¸ìš© - ì£¼ì„ ì²˜ë¦¬)
        # if i < 5:
        #     print(f"\n   [Patent {i}] app_no: {p['app_no']}")
        #     print(f"      count_vector: {count_vector}")
        #     print(f"      text length: {len(text)}")
        #     for j, (keyword, _) in enumerate(weighted_keywords):
        #         print(f"      '{keyword}': {count_vector[j]} times")
        
        # ì „ë¶€ 0ì´ë©´ ì œì™¸
        if all(c == 0 for c in count_vector):
            # if i < 5:
            #     print(f"      âŒ SKIPPED (all zeros)")
            continue
        
        matched_patents += 1
        # if i < 5:
        #     print(f"      âœ… MATCHED!")
        
        scored.append((count_vector, p["app_no"]))
    
    # print(f"\nâœ… [SCAN COMPLETE]")
    # print(f"   Total patents scanned: {len(patent_flattened)}")
    # print(f"   Matched patents: {matched_patents}")
    # print(f"   Match rate: {matched_patents/len(patent_flattened)*100:.2f}%")
    
    if matched_patents == 0:
        # print("\nâŒ [ERROR] No patents matched any keyword!")
        # print("   Possible reasons:")
        # print("   1. Keywords don't exist in patent texts")
        # print("   2. Encoding mismatch (UTF-8 issue)")
        # print("   3. Text normalization issue")
        return []
    
    # âœ… 3. ì‚¬ì „ì‹ ë¹„êµ (ì¤‘ìš” í‚¤ì›Œë“œë¶€í„°)
    scored.sort(key=lambda x: x[0], reverse=True)
    
    # ğŸ” ë””ë²„ê·¸ ì¶œë ¥ (ì£¼ì„ ì²˜ë¦¬)
    # print(f"\nğŸ” [COUNT VECTOR TOP {min(5, len(scored))}]")
    # for vec, app in scored[:5]:
    #     print(f"  vector={vec}, app_no={app}")
    
    result = [app_no for _, app_no in scored[:limit]]
    
    perf_log(f"\nğŸ¯ [FINAL RESULT]")
    perf_log(f"   Returning {len(result)} patents (limit={limit})")
    perf_log(f"   Sample app_nos: {result[:3]}")
    perf_log(f"â±ï¸ [Simple Match ì „ì²´] {time.time() - start:.2f}ì´ˆ")
    perf_log(f"{'='*60}\n")
    
    return result

    
async def hybrid_retrieve(query:str, target_k: int):
    start = time.time()
    
    #ë³‘ë ¬ ì‹¤í–‰ 
    parallel_start = time.time()
    search_apps,qdrant_apps = await asyncio.gather(
        simple_match_search_app_numbers(query, target_k),
        qdrant_search_app_numbers(query, target_k * 2)
    )
    
    perf_log(f"â±ï¸ [ë³‘ë ¬ ê²€ìƒ‰] {time.time() - parallel_start:.2f}ì´ˆ")
    
    s_set = set(search_apps)
    q_set = set(qdrant_apps)
    
    # #1) searchëŠ” ìµœëŒ€ target_k
    # search_apps = simple_match_search_app_numbers(query, target_k)
    # s_set = set(search_apps)
    
    # #2) qdrantëŠ” í•­ìƒ target_k*2 ê°œ ê°€ì ¸ì™€ì„œ search ë¶€ì¡±ë¶„ì„ í™•ì‹¤íˆ ë³´ì™„
    # qdrant_apps = qdrant_search_app_numbers(query, target_k * 2)
    # q_set = set(qdrant_apps)
    
    perf_log(
        f"\nğŸ” [INITIAL RETRIEVAL] â†’ "
        f"search={len(search_apps)}, "
        f"qdrant={len(qdrant_apps)}"
    )
    
    
    used = set()
    docs = []
    
    
    #3) search ìš°ì„  ì¶”ê°€
    for app in search_apps:
        if app not in used and app in patent_index:
            used.add(app)
            docs.append(("MATCH", app, patent_text_index[app]))
            
            
    #4) qdrantë¡œ ë¶€ì¡±ë¶„ ì±„ìš°ê¸°
    for app in qdrant_apps:
        if len (docs) >= target_k * 2:
            break
        if app not in used and app in patent_index:
            used.add(app)
            docs.append(("QDRANT",app,patent_text_index[app]))
            
    
    # ---------- ë¡œê·¸ ê³„ì‚° (ì—¬ê¸°ê°€ í•µì‹¬!) ----------
    final_apps = {app for _, app, _ in docs}   # == len(final_apps) == top_k*2

    overlap = len(final_apps & s_set & q_set)
    search_only = len((final_apps & s_set) - q_set)
    qdrant_only = len((final_apps & q_set) - s_set)

    total_docs = len(final_apps)   # ë°˜ë“œì‹œ top_k*2

    perf_log(
        f"\nğŸ“Š SOURCE STATS â†’ "
        f"overlap={overlap}, "
        f"search_only={search_only}, "
        f"qdrant_only={qdrant_only}, "
        f"total_docs={total_docs}"
    )
    perf_log(f"â±ï¸ [Hybrid Retrieve ì „ì²´] {time.time() - start:.2f}ì´ˆ")
    # ---------------------------------------------

    return docs

def build_prompt(question, context):
    return f"""
ë‹¹ì‹ ì€ í•œì–‘ëŒ€í•™êµ ERICA ì‚°í•™í˜‘ë ¥ë‹¨ì´ ë³´ìœ í•œ íŠ¹í—ˆ ë°ì´í„°ë² ì´ìŠ¤(KIPRIS Detail.json)ë¥¼ ì˜ ì´í•´í•˜ê³  ì‚¬ìš©í•˜ëŠ” ì „ë¬¸ íŠ¹í—ˆ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

RULES:
- CONTEXTë§Œì„ ê·¼ê±°ë¡œ í•˜ê³ , ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ìƒˆë¡œìš´ ì‚¬ì‹¤ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ ê²ƒ.
- CONTEXTë¥¼ ì§ì ‘ ì½ëŠ” ê²ƒì²˜ëŸ¼ ë§í•˜ì§€ ë§ê³ , ì „ë¬¸ê°€ ê´€ì ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
- ì£¼ì–´ì§„ PATENTì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ë§Œì„ ì œê³µí•˜ì„¸ìš”.
- ì£¼ì–´ì§„ PATENTì— ì •í™•í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì•Œ ìˆ˜ ì—†ë‹¤ê³  ë‹µí•˜ì„¸ìš”.
- ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì¡°ê±´ì— ë§ëŠ” ë‚´ìš©ë§Œ ëª…ë£Œí•˜ê²Œ ë‹µí•˜ì„¸ìš”.
- ì§ˆë¬¸ì— ì˜¤íƒ€, ë„ì–´ì“°ê¸° ì˜¤ë¥˜, í•œì˜ë³€í™˜ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ê²ƒì´ ìˆë‹¤ë©´ ì •ì œí•˜ì—¬ ë‹µí•˜ì„¸ìš”.

[CONTEXT]
{context}

[QUESTION]
{question}

[ANSWER]
"""    
            
    
async def hybrid_rag_answer(query:str, top_k: int):
    overall_start = time.time()
    perf_log(f"\n{'#'*70}")
    perf_log(f"ğŸ¤– [RAG ë‹µë³€ ìƒì„± ì‹œì‘] Query: '{query[:50]}...'")
    perf_log(f"{'#'*70}")
    
    # 1. ë¬¸ì„œ ê²€ìƒ‰
    retrieve_start = time.time()
    docs = await hybrid_retrieve(query,top_k)
    retrieve_elapsed = time.time() - retrieve_start
    
    if not docs:
        return "ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    
    perf_log(f"â±ï¸ [1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰] {retrieve_elapsed:.2f}ì´ˆ â†’ {len(docs)}ê°œ ë¬¸ì„œ")
    
    # 2. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context_start = time.time()
    context = ""
    for i, (source, app_no, text) in enumerate(docs):
        context += f"""
\n===========================================================================
ğŸ“„ PATENT {i+1}
APPLICATION_NUMBER: {app_no}
=============================================================================\n
{text}
"""
    context_elapsed = time.time() - context_start
    perf_log(f"â±ï¸ [2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìƒì„±] {context_elapsed:.2f}ì´ˆ â†’ {len(context):,}ì")

    prompt = build_prompt(query, context)

    # 3. LLM ë‹µë³€ ìƒì„±
    llm_start = time.time()
    resp = await client_openai.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": prompt}],
        #temperature=0.2
    )
    llm_elapsed = time.time() - llm_start
    
    answer = resp.choices[0].message.content.strip()
    overall_elapsed = time.time() - overall_start
    
    # ìµœì¢… ìš”ì•½
    perf_log(f"â±ï¸ [3ë‹¨ê³„: LLM ë‹µë³€ ìƒì„±] {llm_elapsed:.2f}ì´ˆ â†’ {len(answer)}ì")
    perf_log(f"\n{'='*70}")
    perf_log(f"âœ… [ì „ì²´ ì™„ë£Œ] {overall_elapsed:.2f}ì´ˆ")
    perf_log(f"   1. ë¬¸ì„œ ê²€ìƒ‰:      {retrieve_elapsed:6.2f}ì´ˆ ({retrieve_elapsed/overall_elapsed*100:5.1f}%)")
    perf_log(f"   2. ì»¨í…ìŠ¤íŠ¸ ìƒì„±:  {context_elapsed:6.2f}ì´ˆ ({context_elapsed/overall_elapsed*100:5.1f}%)")
    perf_log(f"   3. LLM ë‹µë³€:       {llm_elapsed:6.2f}ì´ˆ ({llm_elapsed/overall_elapsed*100:5.1f}%)")
    perf_log(f"{'='*70}\n")

    return answer

#--------------------------------------
#ë°ì´í„° ì´ˆê¸°í™” í•¨ìˆ˜

async def initialize_data():
    global client_openai, client_qdrant, patents, patent_index, patent_text_index, patent_flattened
    
    print("â–¶ Initializing clients...")
    client_openai = AsyncOpenAI(api_key=OPENAI_API_KEY) 
    client_qdrant = AsyncQdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
    print("â–¶ Qdrant Connected")

    print("â–¶ Loading patent data...")
    with open(JSON_PATH, "r", encoding="utf-8") as f:
        patents = json.load(f)
    print(f"â–¶ íŠ¹í—ˆ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(patents)}ê°œ")
    
    # ğŸ” ì²« ë²ˆì§¸ íŠ¹í—ˆ êµ¬ì¡° í™•ì¸ 
    # if patents:
    #     perf_log("\nğŸ” [FIRST PATENT STRUCTURE]")
    #     first_patent = patents[0]
    #     perf_log(f"   Type: {type(first_patent)}")
    #     perf_log(f"   Keys: {list(first_patent.keys()) if isinstance(first_patent, dict) else 'Not a dict'}")
    #     perf_log(f"   JSON preview: {json.dumps(first_patent, ensure_ascii=False, indent=2)[:500]}...")

    print("\nâ–¶ Building indexes...")
    for p in patents:
        raw = extract_application_number(p)
        app_no = normalize_application_number(raw)
        if app_no:
            patent_index[app_no] = p

    print(f"â–¶ applicationNumber index ìƒì„± ì™„ë£Œ: {len(patent_index)}ê°œ")

    # ğŸ” ì²« 3ê°œ íŠ¹í—ˆì—ì„œ ìƒì„¸ ë””ë²„ê¹… (ì£¼ì„ ì²˜ë¦¬)
    # for i, patent in enumerate(patents[:3]):
    #     app_no = normalize_application_number(extract_application_number(patent))
    #     if not app_no:
    #         continue
    #     
    #     perf_log(f"\nğŸ” [PATENT {i+1}] app_no: {app_no}")
    #     
    #     # ê° í•„ë“œê°€ ì°¾ì•„ì§€ëŠ”ì§€ í™•ì¸
    #     title = find_key_recursive(patent, "inventionTitle")
    #     abstract = find_key_recursive(patent, "astrtCont")
    #     claims = find_key_recursive(patent, "claim")
    #     inventors = find_key_recursive(patent, "name")
    #     
    #     perf_log(f"   inventionTitle found: {len(title)} items â†’ {title[:1] if title else 'NONE'}")
    #     perf_log(f"   astrtCont found: {len(abstract)} items â†’ {abstract[:1] if abstract else 'NONE'}")
    #     perf_log(f"   claim found: {len(claims)} items")
    #     perf_log(f"   name found: {len(inventors)} items â†’ {inventors[:3] if inventors else 'NONE'}")
    #     
    #     cleaned_text = build_patent_context_ko(patent)
    #     perf_log(f"   Final text length: {len(cleaned_text)}")
    #     perf_log(f"   Text preview: {cleaned_text[:200]}...")
    #     
    #     patent_text_index[app_no] = cleaned_text
    #     patent_flattened.append({"app_no": app_no, "text": cleaned_text})

    # ëª¨ë“  íŠ¹í—ˆ ì²˜ë¦¬
    for patent in patents:
        app_no = normalize_application_number(extract_application_number(patent))
        if not app_no:
            continue
        cleaned_text = build_patent_context_ko(patent)
        patent_text_index[app_no] = cleaned_text
        patent_flattened.append({"app_no": app_no, "text": cleaned_text})

    print(f"\nâ–¶ patent_flattened size: {len(patent_flattened)}")
    
    # âœ… í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
    if patent_flattened:
        avg_length = sum(len(p["text"]) for p in patent_flattened) / len(patent_flattened)
        min_length = min(len(p["text"]) for p in patent_flattened)
        max_length = max(len(p["text"]) for p in patent_flattened)
        print(f"â–¶ Text length stats: avg={avg_length:.0f}, min={min_length}, max={max_length}")
    
    print("âœ… Initialization complete!")
    
    
    
